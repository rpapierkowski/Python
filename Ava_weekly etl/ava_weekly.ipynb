{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mysql.connector  as connection\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import numpy as np\n",
    "import csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_dir = 'G:/Reports/32_forecast_tool/ava history/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ava = pd.DataFrame({}, columns=['article_id', 'date', 'stock', 'reserved', 'deleted'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_check = pd.read_csv(file_dir + 'ava_weekly.csv')\n",
    "date_column = date_check['date']\n",
    "\n",
    "# Checking if the date column is empty\n",
    "if date_column.empty:\n",
    "    start = '2022-01-03'\n",
    "else:\n",
    "    last_date_str = str(date_column.iloc[-1])\n",
    "    last_date = datetime.datetime.strptime(last_date_str, '%Y-%m-%d').date()\n",
    "    start = (last_date + pd.DateOffset(days=7)).strftime('%Y-%m-%d')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rdpadmin\\AppData\\Local\\Temp\\ipykernel_15768\\134610610.py:31: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  result = pd.read_sql(sql_query,db_connections)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 31\u001b[0m\n\u001b[0;32m     29\u001b[0m sql_file \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(file_dir \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39mava.sql\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     30\u001b[0m sql_query \u001b[39m=\u001b[39m sql_file\u001b[39m.\u001b[39mread()\u001b[39m.\u001b[39mformat(start_date)\n\u001b[1;32m---> 31\u001b[0m result \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_sql(sql_query,db_connections)\n\u001b[0;32m     33\u001b[0m \u001b[39m# Union result with ava\u001b[39;00m\n\u001b[0;32m     34\u001b[0m ava \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mconcat([ava, result], axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\rdpadmin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\sql.py:633\u001b[0m, in \u001b[0;36mread_sql\u001b[1;34m(sql, con, index_col, coerce_float, params, parse_dates, columns, chunksize, dtype_backend, dtype)\u001b[0m\n\u001b[0;32m    631\u001b[0m \u001b[39mwith\u001b[39;00m pandasSQL_builder(con) \u001b[39mas\u001b[39;00m pandas_sql:\n\u001b[0;32m    632\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(pandas_sql, SQLiteDatabase):\n\u001b[1;32m--> 633\u001b[0m         \u001b[39mreturn\u001b[39;00m pandas_sql\u001b[39m.\u001b[39;49mread_query(\n\u001b[0;32m    634\u001b[0m             sql,\n\u001b[0;32m    635\u001b[0m             index_col\u001b[39m=\u001b[39;49mindex_col,\n\u001b[0;32m    636\u001b[0m             params\u001b[39m=\u001b[39;49mparams,\n\u001b[0;32m    637\u001b[0m             coerce_float\u001b[39m=\u001b[39;49mcoerce_float,\n\u001b[0;32m    638\u001b[0m             parse_dates\u001b[39m=\u001b[39;49mparse_dates,\n\u001b[0;32m    639\u001b[0m             chunksize\u001b[39m=\u001b[39;49mchunksize,\n\u001b[0;32m    640\u001b[0m             dtype_backend\u001b[39m=\u001b[39;49mdtype_backend,  \u001b[39m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m    641\u001b[0m             dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[0;32m    642\u001b[0m         )\n\u001b[0;32m    644\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    645\u001b[0m         _is_table_name \u001b[39m=\u001b[39m pandas_sql\u001b[39m.\u001b[39mhas_table(sql)\n",
      "File \u001b[1;32mc:\\Users\\rdpadmin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\sql.py:2279\u001b[0m, in \u001b[0;36mSQLiteDatabase.read_query\u001b[1;34m(self, sql, index_col, coerce_float, parse_dates, params, chunksize, dtype, dtype_backend)\u001b[0m\n\u001b[0;32m   2268\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_query_iterator(\n\u001b[0;32m   2269\u001b[0m         cursor,\n\u001b[0;32m   2270\u001b[0m         chunksize,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2276\u001b[0m         dtype_backend\u001b[39m=\u001b[39mdtype_backend,\n\u001b[0;32m   2277\u001b[0m     )\n\u001b[0;32m   2278\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 2279\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fetchall_as_list(cursor)\n\u001b[0;32m   2280\u001b[0m     cursor\u001b[39m.\u001b[39mclose()\n\u001b[0;32m   2282\u001b[0m     frame \u001b[39m=\u001b[39m _wrap_result(\n\u001b[0;32m   2283\u001b[0m         data,\n\u001b[0;32m   2284\u001b[0m         columns,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2289\u001b[0m         dtype_backend\u001b[39m=\u001b[39mdtype_backend,\n\u001b[0;32m   2290\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\rdpadmin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\sql.py:2294\u001b[0m, in \u001b[0;36mSQLiteDatabase._fetchall_as_list\u001b[1;34m(self, cur)\u001b[0m\n\u001b[0;32m   2293\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_fetchall_as_list\u001b[39m(\u001b[39mself\u001b[39m, cur):\n\u001b[1;32m-> 2294\u001b[0m     result \u001b[39m=\u001b[39m cur\u001b[39m.\u001b[39;49mfetchall()\n\u001b[0;32m   2295\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(result, \u001b[39mlist\u001b[39m):\n\u001b[0;32m   2296\u001b[0m         result \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(result)\n",
      "File \u001b[1;32mc:\\Users\\rdpadmin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\mysql\\connector\\cursor_cext.py:613\u001b[0m, in \u001b[0;36mCMySQLCursor.fetchall\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    610\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cnx\u001b[39m.\u001b[39munread_result:\n\u001b[0;32m    611\u001b[0m     \u001b[39mreturn\u001b[39;00m []\n\u001b[1;32m--> 613\u001b[0m rows: Tuple[List[RowType], Optional[CextEofPacketType]] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_cnx\u001b[39m.\u001b[39;49mget_rows()\n\u001b[0;32m    614\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_nextrow \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_nextrow[\u001b[39m0\u001b[39m]:\n\u001b[0;32m    615\u001b[0m     rows[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39minsert(\u001b[39m0\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_nextrow[\u001b[39m0\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\rdpadmin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\mysql\\connector\\connection_cext.py:428\u001b[0m, in \u001b[0;36mCMySQLConnection.get_rows\u001b[1;34m(self, count, binary, columns, raw, prep_stmt)\u001b[0m\n\u001b[0;32m    426\u001b[0m     \u001b[39mif\u001b[39;00m count \u001b[39mand\u001b[39;00m counter \u001b[39m==\u001b[39m count:\n\u001b[0;32m    427\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m--> 428\u001b[0m     row \u001b[39m=\u001b[39m fetch_row()\n\u001b[0;32m    429\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m row:\n\u001b[0;32m    430\u001b[0m     _eof: Optional[CextEofPacketType] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfetch_eof_columns(prep_stmt)[\n\u001b[0;32m    431\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39meof\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    432\u001b[0m     ]  \u001b[39m# type: ignore[assignment]\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Set a date of first update\n",
    "start_date = pd.to_datetime(start).date()\n",
    "\n",
    "# Set date when we want to end the process\n",
    "today = datetime.date.today()\n",
    "offset = (today.weekday() - 0) % 7\n",
    "first_day_of_currweek = today - datetime.timedelta(days=offset)\n",
    "\n",
    "# Set period you want to process\n",
    "end_date = pd.to_datetime((start_date+ pd.DateOffset(days=7)).date()).date()\n",
    "\n",
    "while start_date <= end_date: #first_day_of_currweek: \n",
    "\n",
    "    if start_date > first_day_of_currweek:\n",
    "        break\n",
    "\n",
    "    # Run MySQL query to fetch data\n",
    "    # Putty must be running while running the script\n",
    "    with open(file_dir + 'db_credentials.txt', 'r') as f:\n",
    "        db_credentials = f.read().splitlines()\n",
    "\n",
    "    try:\n",
    "        db_connections = connection.connect(\n",
    "            host=db_credentials[0],\n",
    "            user=db_credentials[1],\n",
    "            password=db_credentials[2],\n",
    "            database=db_credentials[3]\n",
    "        )\n",
    "        sql_file = open(file_dir + 'ava.sql')\n",
    "        sql_query = sql_file.read().format(start_date)\n",
    "        result = pd.read_sql(sql_query,db_connections)\n",
    "\n",
    "        # Union result with ava\n",
    "        ava = pd.concat([ava, result], axis=0)\n",
    "\n",
    "        # close the connection\n",
    "        db_connections.close()  \n",
    "    except Exception as e:\n",
    "        db_connections.close()\n",
    "        print(str(e))\n",
    "\n",
    "    # Add 1 week to previously updated\n",
    "    start_date = (start_date+ pd.DateOffset(days=7)).date() \n",
    "\n",
    "print('Script is fully processed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rdpadmin\\AppData\\Local\\Temp\\ipykernel_16480\\957522657.py:14: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  sales = pd.read_sql(sql_query2,db_connections)\n"
     ]
    }
   ],
   "source": [
    "# Putty must be running while running the script\n",
    "with open(file_dir + 'db_credentials.txt', 'r') as f:\n",
    "    db_credentials = f.read().splitlines()\n",
    "\n",
    "try:\n",
    "    db_connections = connection.connect(\n",
    "        host=db_credentials[0],\n",
    "        user=db_credentials[1],\n",
    "        password=db_credentials[2],\n",
    "        database=db_credentials[3]\n",
    "    )\n",
    "    sql_file2 = open(file_dir + 'sales.sql')\n",
    "    sql_query2 = sql_file2.read()\n",
    "    sales = pd.read_sql(sql_query2,db_connections)\n",
    "\n",
    "    # Close the connection\n",
    "    db_connections.close()  \n",
    "except Exception as e:\n",
    "    db_connections.close()\n",
    "    print(str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ava['key_column'] = pd.to_datetime(ava['date']).astype(np.int64) // 10**9\n",
    "ava['key_column'] = ava['key_column'].astype(str) + '-' + ava['article_id'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales['date2'] = pd.to_datetime(sales['date']).dt.to_period('W').dt.start_time\n",
    "sales = sales.groupby(['date2', 'article_id']).size().reset_index(name='sale')\n",
    "sales['key_column'] = pd.to_datetime(sales['date2']).astype(np.int64) // 10**9\n",
    "sales['key_column'] = sales['key_column'].astype(str) + '-' + sales['article_id'].astype(str)\n",
    "sales = sales.rename(columns={'date2': 'date'})\n",
    "sales = sales.drop(['date', 'article_id'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ava_full = pd.merge(ava, sales, on='key_column', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ava_full['sale'] = ava_full['sale'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ava_full = ava_full.drop(['key_column'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ava_full['sale'] = ava_full['sale'].astype(str)\n",
    "ava_full['sale'] = ava_full['sale'].replace(r'\\.0', '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If it's all good, write the result into the file\n",
    "with open(file_dir + 'ava_weekly.csv', 'a', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    if csvfile.tell() == 0:  # Check if the file is empty\n",
    "        writer.writerow(result.columns)  # Write header row if file is empty\n",
    "\n",
    "    ava_full.to_csv(csvfile, index=False, header=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
