{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mysql.connector  as connection\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_dir = 'C:/Users/User/Desktop/Moje dokumenty/Raporty obce/Kacper/63_issue_158946_payments/'\n",
    "curr_dir = 'C:/Users/User/Desktop/Moje dokumenty/Raporty obce/Kacper/Curr_rate/'\n",
    "save_directory = 'C:/Users/User/Desktop/Moje dokumenty/Raporty obce/Kacper/63_issue_158946_payments/python'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'files_dir' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[143], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Adding existing files\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m DIM_auftrag_temp \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(files_dir \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39mall_fact_table.csv\u001b[39m\u001b[39m'\u001b[39m )\n\u001b[0;32m      3\u001b[0m shiping_join_fact_table_v2 \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(files_dir \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39mshiping_join_fact_table.csv\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m payments_2019_2022_Sylwia_temp_all \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(files_dir \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39mall_union.csv\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'files_dir' is not defined"
     ]
    }
   ],
   "source": [
    "# Adding existing files\n",
    "DIM_auftrag_temp = pd.read_csv(files_dir + 'all_fact_table.csv' )\n",
    "shiping_join_fact_table_v2 = pd.read_csv(files_dir + 'shiping_join_fact_table.csv')\n",
    "payments_2019_2022_Sylwia_temp_all = pd.read_csv(files_dir + 'all_union.csv')\n",
    "\n",
    "# Values from swiss website. Need to run 'Curr_rate' script to refresh.\n",
    "# This part probably has to be changed to get values from DB\n",
    "Curr_rate_website = pd.read_csv(curr_dir + 'Curr_rate_website_all.csv')\n",
    "Curr_rate = pd.read_csv(curr_dir + 'Curr_rate/Curr_rate.csv')\n",
    "\n",
    "\n",
    "print('CSV files are loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading ticket_payment_2022_refresh ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_5844\\2741318555.py:15: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  shipping_2022_refresh = pd.read_sql(shipping_issue_158946_query,db_connections)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shipping_2022_refresh is ready\n",
      "All MySQL queries are loaded and db_connections is closed \n"
     ]
    }
   ],
   "source": [
    "with open(files_dir + 'db_credentials.txt', 'r') as f:\n",
    "    db_credentials = f.read().splitlines()\n",
    "\n",
    "try:\n",
    "    db_connections = connection.connect(\n",
    "        host=db_credentials[0],\n",
    "        user=db_credentials[1],\n",
    "        password=db_credentials[2],\n",
    "        database=db_credentials[3]\n",
    "    )\n",
    "        # loading shippings\n",
    "    print(\"loading ticket_payment_2022_refresh ...\")\n",
    "    shipping_issue_158946 = open(files_dir + 'shipping_issue_158946.sql')\n",
    "    shipping_issue_158946_query = shipping_issue_158946.read()\n",
    "    shipping_2022_refresh = pd.read_sql(shipping_issue_158946_query,db_connections)\n",
    "    print('shipping_2022_refresh is ready')\n",
    "\n",
    "    # close the connection\n",
    "    db_connections.close()  \n",
    "    print('All MySQL queries are loaded and db_connections is closed ')\n",
    "except Exception as e:\n",
    "    db_connections.close()\n",
    "    print(str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading Auftrag_2022_refresh ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_5844\\862448662.py:16: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  Auftrag_2022_refresh = pd.read_sql(dim_auftrag_query,db_connections)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Auftrag_2022_refresh is ready\n",
      "loading cost_2022_refresh ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_5844\\862448662.py:23: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  cost_2022_refresh = pd.read_sql(cost_issue_158946_query,db_connections)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost_2022_refresh is ready\n",
      "loading ticket_payment_2022_refresh ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_5844\\862448662.py:30: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  ticket_payment_2022_refresh = pd.read_sql(ticket_payment_union_query,db_connections)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ticket_payment_2022_refresh is ready\n",
      "loading ticket_payment_2022_refresh ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_5844\\862448662.py:37: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  shipping_2022_refresh = pd.read_sql(shipping_issue_158946_query,db_connections)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shipping_2022_refresh is ready\n",
      "All MySQL queries are loaded and db_connections is closed \n"
     ]
    }
   ],
   "source": [
    "# Run MySQL query to fetch auftrags. To refresh data, keep in mind to set correct dates in MySQL query\n",
    "# Putty must be running while running the script\n",
    "with open(files_dir + 'db_credentials.txt', 'r') as f:\n",
    "    db_credentials = f.read().splitlines()\n",
    "\n",
    "try:\n",
    "    db_connections = connection.connect(\n",
    "        host=db_credentials[0],\n",
    "        user=db_credentials[1],\n",
    "        password=db_credentials[2],\n",
    "        database=db_credentials[3]\n",
    "    )\n",
    "    # loading auftrags\n",
    "    print(\"loading Auftrag_2022_refresh ...\")\n",
    "    dim_auftrag = open(files_dir + 'Dim_auftrag.sql')\n",
    "    dim_auftrag_query = dim_auftrag.read()\n",
    "    Auftrag_2022_refresh = pd.read_sql(dim_auftrag_query,db_connections)\n",
    "    print('Auftrag_2022_refresh is ready')\n",
    "\n",
    "    # loading cost\n",
    "    print(\"loading cost_2022_refresh ...\")\n",
    "    cost_issue_158946 = open(files_dir + 'cost_issue_158946.sql')\n",
    "    cost_issue_158946_query = cost_issue_158946.read()\n",
    "    cost_2022_refresh = pd.read_sql(cost_issue_158946_query,db_connections)\n",
    "    print('cost_2022_refresh is ready')\n",
    "\n",
    "    # loading tickets\n",
    "    print(\"loading ticket_payment_2022_refresh ...\")\n",
    "    ticket_payment_union = open(files_dir + 'Ticket_payment_union.sql')\n",
    "    ticket_payment_union_query = ticket_payment_union.read()\n",
    "    ticket_payment_2022_refresh = pd.read_sql(ticket_payment_union_query,db_connections)\n",
    "    print('ticket_payment_2022_refresh is ready')\n",
    "\n",
    "    # loading shippings\n",
    "    print(\"loading ticket_payment_2022_refresh ...\")\n",
    "    shipping_issue_158946 = open(files_dir + 'shipping_issue_158946.sql')\n",
    "    shipping_issue_158946_query = shipping_issue_158946.read()\n",
    "    shipping_2022_refresh = pd.read_sql(shipping_issue_158946_query,db_connections)\n",
    "    print('shipping_2022_refresh is ready')\n",
    "\n",
    "    # close the connection\n",
    "    db_connections.close()  \n",
    "    print('All MySQL queries are loaded and db_connections is closed ')\n",
    "except Exception as e:\n",
    "    db_connections.close()\n",
    "    print(str(e))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the number of duplicate Order_IDs\n",
    "num_duplicates = sum(cost_2022_refresh.duplicated(subset='Order_ID'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset the rows with duplicated Order_IDs\n",
    "duplicated_rows = cost_2022_refresh[cost_2022_refresh.duplicated(subset='Order_ID')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Keep the row with the earliest Shipped_date for each unique Order_ID\n",
    "cost_2022_refresh = cost_2022_refresh.groupby('Order_ID').filter(lambda x: x['Shipped_date'].eq(x['Shipped_date'].min()).any()).drop_duplicates(subset='Order_ID', keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a left join on the cost_2022_refresh and shipping_2022_refresh DataFrames\n",
    "shiping_join = cost_2022_refresh.merge(shipping_2022_refresh.drop('Shipped_date', axis=1), \n",
    "                                       how='left', \n",
    "                                       left_on=['MAU_Main_Auction', 'Order_ID', 'Article_ID', 'Main Auction'], \n",
    "                                       right_on=['MAU Main Auction', 'Order_id', 'Article_ID', 'Main Auction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create CHF to EUR table\n",
    "\n",
    "Curr_rate_web_eur = Curr_rate_website.loc[Curr_rate_website['Currency'] == 'EUR']\n",
    "Curr_rate_web_eur = Curr_rate_web_eur.drop(['Quantity', 'Currency'], axis=1)\n",
    "Curr_rate_web_eur = Curr_rate_web_eur.rename(columns={'Exchange': 'Exchange_EUR'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning Auftrag_2022_refresh variable\n",
    "\n",
    "Auftrag_2022_refresh['Auftrag_create_date'] = pd.to_datetime(Auftrag_2022_refresh['Auftrag_create_date'])\n",
    "Auftrag_2022_refresh['Invoice_date'] = pd.to_datetime(Auftrag_2022_refresh['Invoice_date'])\n",
    "\n",
    "Auftrag_2022_refresh['Company'] = np.select(\n",
    "    [\n",
    "        Auftrag_2022_refresh['Seller'] == 'Beliani PL',\n",
    "        Auftrag_2022_refresh['Seller'].isin(['BCE', 'Beliani (International) GmbH']),\n",
    "        Auftrag_2022_refresh['Seller'].isin(['Beliani UK', 'Design_UK']),\n",
    "        Auftrag_2022_refresh['Seller'].isin(['Beliani', 'ayohama', 'Ricardo', 'CH: Express24', 'Allegro Broken PL']),\n",
    "        Auftrag_2022_refresh['Seller'] == 'Artehome'\n",
    "    ],\n",
    "    [\n",
    "        'Beliani PL',\n",
    "        'Broken items',\n",
    "        'Beliani UK',\n",
    "        'Beliani CH',\n",
    "        'Schoenteakmoebel'\n",
    "    ],\n",
    "    default='Beliani DE'\n",
    ")\n",
    "\n",
    "Auftrag_2022_refresh['Currency'] = np.select(\n",
    "    [\n",
    "        Auftrag_2022_refresh['Sellers country'] == 'DK',\n",
    "        Auftrag_2022_refresh['Sellers country'] == 'HU',\n",
    "        Auftrag_2022_refresh['Sellers country'] == 'NO',\n",
    "        Auftrag_2022_refresh['Sellers country'] == 'SE',\n",
    "        Auftrag_2022_refresh['Sellers country'] == 'PL',\n",
    "        Auftrag_2022_refresh['Sellers country'] == 'GB',\n",
    "        Auftrag_2022_refresh['Sellers country'] == 'CH',\n",
    "        Auftrag_2022_refresh['Sellers country'] == 'CZ',\n",
    "        Auftrag_2022_refresh['Sellers country'] == 'PB'\n",
    "    ],\n",
    "    [\n",
    "        'DKK',\n",
    "        'HUF',\n",
    "        'NOK',\n",
    "        'SEK',\n",
    "        'PLN',\n",
    "        'GBP',\n",
    "        'CHF',\n",
    "        'CZK',\n",
    "        'PLN'\n",
    "    ],\n",
    "    default='EUR'\n",
    ")\n",
    "\n",
    "Auftrag_2022_refresh['Auftrag_link'] = 'https://www.prologistics.info/auction.php?number=' + Auftrag_2022_refresh['Auction number'].astype(str) + '&txnid=' + Auftrag_2022_refresh['Txnid'].astype(str)\n",
    "\n",
    "Auftrag_2022_refresh = Auftrag_2022_refresh.dropna(subset=['open_amount'])\n",
    "\n",
    "Curr_rate_website = Curr_rate_website.loc[(Curr_rate_website['Year'] == Curr_rate_website['Year'].max()) & (Curr_rate_website['Month'] == Curr_rate_website['Month'].max()), :]\n",
    "Auftrag_2022_refresh = pd.merge(Auftrag_2022_refresh, Curr_rate_website, on='Currency', how='left')\n",
    "\n",
    "Auftrag_2022_refresh['open_amount_CHF'] = np.where(Auftrag_2022_refresh['Currency'] == 'CHF',\n",
    "                                                  Auftrag_2022_refresh['open_amount'],\n",
    "                                                  Auftrag_2022_refresh['open_amount'] / Auftrag_2022_refresh['Quantity'] * Auftrag_2022_refresh['Exchange'])\n",
    "\n",
    "Auftrag_2022_refresh['open_amount_CHF_netto'] = np.where(Auftrag_2022_refresh['Currency'] == 'CHF',\n",
    "                                                         Auftrag_2022_refresh['Open_amount_netto'],\n",
    "                                                         Auftrag_2022_refresh['Open_amount_netto'] / Auftrag_2022_refresh['Quantity'] * Auftrag_2022_refresh['Exchange'])\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get latest shipping date for each auction in the shiping_join data frame\n",
    "\n",
    "Shiping_max_date = shiping_join[['MAU_Main_Auction', 'Shipped_date']].groupby('MAU_Main_Auction').max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete from the data frame subauftrags which has been deleted\n",
    "\n",
    "Shiping_sub_deleted = shiping_join.loc[:, ['MAU_Main_Auction', 'Main Auction', 'Order_not_sent', 'Auftrag_deleted']]\n",
    "Shiping_sub_deleted = Shiping_sub_deleted[Shiping_sub_deleted['Main Auction'] != Shiping_sub_deleted['MAU_Main_Auction']]\n",
    "Shiping_sub_deleted = Shiping_sub_deleted[Shiping_sub_deleted['Auftrag_deleted'] == 1]\n",
    "Shiping_sub_deleted = Shiping_sub_deleted.drop(columns=['Auftrag_deleted'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum Order_not_sent for MAU_Main_Auction\n",
    "\n",
    "Shiping_sum_orders_not_sent = (\n",
    "    shiping_join[['MAU_Main_Auction', 'Order_not_sent']]\n",
    "    .merge(\n",
    "        Shiping_sub_deleted[['MAU_Main_Auction']],\n",
    "        on='MAU_Main_Auction',\n",
    "        how='left',\n",
    "        indicator=True\n",
    "    )\n",
    "    .query(\"_merge == 'left_only'\")\n",
    "    .groupby('MAU_Main_Auction')\n",
    "    .agg({'Order_not_sent': 'sum'})\n",
    "    .reset_index()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join above to Auftrag_2022_refresh\n",
    "\n",
    "Auftrag_2022_refresh = pd.merge(Auftrag_2022_refresh, Shiping_max_date, left_on='MAU_Main auction', right_on='MAU_Main_Auction', how='left')\n",
    "\n",
    "Auftrag_2022_refresh = pd.merge(Auftrag_2022_refresh, Shiping_sum_orders_not_sent, left_on='MAU_Main auction', right_on='MAU_Main_Auction', how='left')\n",
    "\n",
    "Auftrag_2022_refresh['Shipped_date'] = pd.to_datetime(Auftrag_2022_refresh['Shipped_date'])\n",
    "\n",
    "Auftrag_2022_refresh['Order_not_sent'] = pd.to_numeric(Auftrag_2022_refresh['Order_not_sent'])\n",
    "Auftrag_2022_refresh['Order_not_sent'] = Auftrag_2022_refresh.apply(lambda row: 1 if pd.isna(row['Order_not_sent']) and row['deleted'] == 1 else row['Order_not_sent'], axis=1)\n",
    "\n",
    "Auftrag_2022_refresh = Auftrag_2022_refresh.dropna(subset=['Order_not_sent'])\n",
    "\n",
    "Auftrag_2022_refresh = Auftrag_2022_refresh.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove deleted auftrags without payment \n",
    "\n",
    "DIM_auftrag_deleted_keep = Auftrag_2022_refresh[\n",
    "    (Auftrag_2022_refresh['deleted'] == 1) & \n",
    "    (Auftrag_2022_refresh['MAU_Main auction'].isin(ticket_payment_2022_refresh['Main auction']))\n",
    "]\n",
    "Auftrag_2022_refresh = pd.concat([Auftrag_2022_refresh[Auftrag_2022_refresh['deleted'] == 0], DIM_auftrag_deleted_keep])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete deleted subinvoices\n",
    "\n",
    "DIM_auftrag_deleted_subs = Auftrag_2022_refresh[(Auftrag_2022_refresh['deleted'] == 1) & (Auftrag_2022_refresh['MAU_Main auction'] != Auftrag_2022_refresh['Main auction'])]\n",
    "Auftrag_2022_refresh = Auftrag_2022_refresh[~Auftrag_2022_refresh.isin(DIM_auftrag_deleted_subs)].dropna()\n",
    "\n",
    "DIM_auftrag_temp_to_move = Auftrag_2022_refresh[Auftrag_2022_refresh['deleted'] == 1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deleted as not sent - because in theory they are not sent even if are stored as sent\n",
    "\n",
    "\n",
    "Auftrag_2022_refresh['Order_not_sent'] = Auftrag_2022_refresh.apply(lambda x: 1 if x['Main auction'] in DIM_auftrag_temp_to_move['Main auction'].values else x['Order_not_sent'], axis=1)\n",
    "\n",
    "Auftrag_2022_refresh['open_amount'] = Auftrag_2022_refresh.apply(lambda x: 0 if x['Main auction'] in DIM_auftrag_temp_to_move['Main auction'].values else x['open_amount'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out current and last year.Earlier years should be a constatnt so we do not need to fetch it. \n",
    "\n",
    "current_year = datetime.datetime.now().year\n",
    "\n",
    "Auftrag_2022_refresh = Auftrag_2022_refresh.drop_duplicates(subset='Main auction', keep='first')\n",
    "\n",
    "Auftrag_2022_refresh = Auftrag_2022_refresh[pd.DatetimeIndex(Auftrag_2022_refresh['Auftrag_create_date']).year >= (current_year - 1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating All_auftrag variable\n",
    "All_auftrag = pd.merge(DIM_auftrag_temp, Auftrag_2022_refresh[['Main auction']], on='Main auction', how='outer', indicator=True)\n",
    "All_auftrag = All_auftrag[All_auftrag['_merge'] == 'left_only'].drop('_merge', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change column type \n",
    "\n",
    "All_auftrag['Invoice_date'] = pd.to_datetime(All_auftrag['Invoice_date'])\n",
    "All_auftrag['Auftrag_create_date'] = pd.to_datetime(All_auftrag['Auftrag_create_date'])\n",
    "All_auftrag['Shipped_date'] = pd.to_datetime(All_auftrag['Shipped_date'])\n",
    "\n",
    "All_auftrag = pd.concat([All_auftrag, Auftrag_2022_refresh], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of Main auction duplicates equals: 0\n"
     ]
    }
   ],
   "source": [
    "# Checking for duplicates. At this point total number of 'Main auction' duplicates should equal 0\n",
    "\n",
    "duplicate_check = sum(All_auftrag.duplicated(['Main auction']))\n",
    "print('Total number of Main auction duplicates equals:', duplicate_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning ticket_payment_2022_refresh variable\n",
    "\n",
    "ticket_payment_2022_refresh['Year'] = pd.DatetimeIndex(ticket_payment_2022_refresh['Transaction_Date']).year\n",
    "ticket_payment_2022_refresh['Month'] = pd.DatetimeIndex(ticket_payment_2022_refresh['Transaction_Date']).month\n",
    "ticket_payment_2022_refresh['Company'] = ticket_payment_2022_refresh['Seller'].replace({\n",
    "    \"Beliani PL\": \"Beliani PL\",\n",
    "    \"BCE\": \"Broken items\",\n",
    "    \"Beliani (International) GmbH\": \"Broken items\",\n",
    "    \"Beliani UK\": \"Beliani UK\",\n",
    "    \"Design_UK\": \"Beliani UK\",\n",
    "    \"Beliani\": \"Beliani CH\",\n",
    "    \"ayohama\": \"Beliani CH\",\n",
    "    \"Ricardo\": \"Beliani CH\",\n",
    "    \"CH: Express24\": \"Beliani CH\",\n",
    "    \"Allegro Broken PL\": \"Beliani CH\",\n",
    "    \"Artehome\": \"Schoenteakmoebel\",\n",
    "        }).fillna(\"Beliani DE\")\n",
    "ticket_payment_2022_refresh['Currency'] = ticket_payment_2022_refresh['Sellers country'].replace({\n",
    "    \"DK\" : \"DKK\",\n",
    "    \"HU\" : \"HUF\",\n",
    "    \"NO\" : \"NOK\",\n",
    "    \"SE\" : \"SEK\",\n",
    "    \"PL\" : \"PLN\",\n",
    "    \"GB\" : \"GBP\",\n",
    "    \"CH\" : \"CHF\",\n",
    "    \"CZ\" : \"CZK\",\n",
    "    \"PB\" : \"PLN\",\n",
    "        }).fillna(\"EUR\")\n",
    "\n",
    "ticket_payment_2022_refresh = pd.merge(ticket_payment_2022_refresh, Curr_rate_website, on=[\"Currency\", \"Month\", \"Year\"], how=\"left\")\n",
    "ticket_payment_2022_refresh[\"Amount_netto(CHF)\"] = np.where(ticket_payment_2022_refresh[\"Currency\"] == 'CHF', ticket_payment_2022_refresh[\"Amount_Netto(local)\"], ticket_payment_2022_refresh[\"Amount_Netto(local)\"] / ticket_payment_2022_refresh[\"Quantity\"] * ticket_payment_2022_refresh[\"Exchange\"])\n",
    "ticket_payment_2022_refresh = pd.merge(ticket_payment_2022_refresh, Curr_rate_web_eur, on=[\"Month\", \"Year\"], how=\"left\")\n",
    "ticket_payment_2022_refresh[\"Amount_netto(EUR)\"] = np.where(ticket_payment_2022_refresh[\"Currency\"] == 'EUR', ticket_payment_2022_refresh[\"Amount_Netto(local)\"], ticket_payment_2022_refresh[\"Amount_netto(CHF)\"] * ticket_payment_2022_refresh[\"Exchange_EUR\"])\n",
    "ticket_payment_2022_refresh[\"Ticket_link\"] = np.where(ticket_payment_2022_refresh[\"Refund?[0-no,1-del_auf,2-ticket]\"] == 2, \"https://www.prologistics.info/rma.php?rma_id=\" + ticket_payment_2022_refresh[\"Payment_id/Ticket_id\"].astype(str) + \"&number=\" + ticket_payment_2022_refresh[\"Auction number\"].astype(str) + \"&txnid=\" + ticket_payment_2022_refresh[\"Txnid\"].astype(str), '')\n",
    "ticket_payment_2022_refresh = ticket_payment_2022_refresh.drop([\"Quantity\", \"Exchange\", \"Exchange_EUR\"], axis=1)\n",
    "ticket_payment_2022_refresh = pd.merge(ticket_payment_2022_refresh, Shiping_max_date, left_on=\"Main auction\", right_on=\"MAU_Main_Auction\", how=\"left\")\n",
    "ticket_payment_2022_refresh[\"Create_date\"] = pd.to_datetime(ticket_payment_2022_refresh[\"Create_date\"])\n",
    "ticket_payment_2022_refresh[\"Shipped_date\"] = pd.to_datetime(ticket_payment_2022_refresh[\"Shipped_date\"])\n",
    "ticket_payment_2022_refresh[\"Ship_create_date\"] = np.where(ticket_payment_2022_refresh[\"Shipped_date\"].isna(), ticket_payment_2022_refresh[\"Create_date\"], ticket_payment_2022_refresh[\"Shipped_date\"])\n",
    "ticket_payment_2022_refresh[\"Transaction_Date\"] = pd.to_datetime(ticket_payment_2022_refresh[\"Transaction_Date\"])\n",
    "ticket_payment_2022_refresh[\"Amount_netto(CHF)_web\"] = ticket_payment_2022_refresh[\"Amount_netto(CHF)\"]\n",
    "ticket_payment_2022_refresh[\"in_GL\"] = 0\n",
    "ticket_payment_2022_refresh = ticket_payment_2022_refresh.drop_duplicates()\n",
    "\n",
    "# Drop not needed columns \n",
    "ticket_payment_2022_refresh = ticket_payment_2022_refresh.drop(columns=['Auction number', 'Txnid', 'Seller', 'Sellers country', 'Customer ID', 'Company', 'Currency'])\n",
    "ticket_payment_2022_refresh['Account'] = ticket_payment_2022_refresh['Account'].astype(str)\n",
    "ticket_payment_2022_refresh = ticket_payment_2022_refresh[ticket_payment_2022_refresh['Transaction_Date'].dt.year >= (pd.Timestamp.now().year - 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# payments_2019_2022_Sylwia_temp_all transformation\n",
    "\n",
    "payments_2019_2022_Sylwia_temp_all = payments_2019_2022_Sylwia_temp_all.loc[payments_2019_2022_Sylwia_temp_all['Transaction_Date'] < '2022-01-01']\n",
    "payments_2019_2022_Sylwia_temp_all['Create_date'] = pd.to_datetime(payments_2019_2022_Sylwia_temp_all['Create_date'])\n",
    "payments_2019_2022_Sylwia_temp_all['Transaction_Date'] = pd.to_datetime(payments_2019_2022_Sylwia_temp_all['Transaction_Date'])\n",
    "payments_2019_2022_Sylwia_temp_all['Shipped_date'] = pd.to_datetime(payments_2019_2022_Sylwia_temp_all['Shipped_date'])\n",
    "payments_2019_2022_Sylwia_temp_all['Ship_create_date'] = pd.to_datetime(payments_2019_2022_Sylwia_temp_all['Ship_create_date'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating All_ticket_payment\n",
    "\n",
    "All_ticket_payment = payments_2019_2022_Sylwia_temp_all.loc[payments_2019_2022_Sylwia_temp_all['Main auction'].isin(DIM_auftrag_temp['MAU_Main auction'])]\n",
    "All_ticket_payment = All_ticket_payment.loc[All_ticket_payment['Year'] <= (pd.Timestamp.now().year-2)]\n",
    "All_ticket_payment = pd.concat([All_ticket_payment, ticket_payment_2022_refresh], ignore_index=True)\n",
    "All_ticket_payment = All_ticket_payment.loc[All_ticket_payment['Main auction'].isin(All_auftrag['MAU_Main auction'])].drop_duplicates()\n",
    "All_ticket_payment['Payment_id/Ticket_id'] = All_ticket_payment['Payment_id/Ticket_id'].astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(save_directory):\n",
    "    os.makedirs(save_directory)\n",
    "\n",
    "All_ticket_payment.to_csv(save_directory + '/All_ticket_payment.csv', index=False, encoding='utf-8')\n",
    "All_auftrag.to_csv(save_directory + '/All_auftrag.csv', index=False, encoding='utf-8')\n",
    "shiping_join.to_csv(save_directory + '/shiping_join.csv', index=False, encoding='utf-8')\n",
    "shiping_join_fact_table_v2.to_csv(save_directory + '/shiping_join_fact_table.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\"missing\" part of the script to be added"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
